{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11647542,"sourceType":"datasetVersion","datasetId":7309150}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Financial Transactions Fraud Detection: Exploratory Data Analysis\n\n## 1. Introduction\n\n### This notebook explores the Financial Transactions Dataset to identify patterns in fraudulent transactions. The dataset includes transaction details, fraud labels, and anomaly scores, making it ideal for fraud detection modeling.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Data Loading and Inspection","metadata":{}},{"cell_type":"code","source":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom scipy import stats\n\n# Scikit-learn imports\nfrom sklearn.experimental import enable_halving_search_cv  # Required for HalvingRandomSearchCV\nfrom sklearn.model_selection import train_test_split, HalvingRandomSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (classification_report, roc_auc_score, \n                            average_precision_score, confusion_matrix)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import resample\n\n# Set visualization style\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('husl')\nplt.rcParams['figure.figsize'] = (10, 6)\nplt.rcParams['font.size'] = 12\n\n# Set visualization style\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('husl')\nplt.rcParams['figure.figsize'] = (10, 6)\nplt.rcParams['font.size'] = 12\n\n# Load the dataset\ndata = pd.read_csv('/kaggle/input/financial-transactions-dataset-for-fraud-detection/financial_fraud_detection_dataset.csv')\n\n# Display the first few rows\ndata.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Overview\n\nBased on data.head(), the dataset has the following columns:\n\n1. transaction_id: Unique identifier for each transaction (string).\n2. timestamp: Transaction datetime (e.g., “2023-08-22T09:22:43.516168”).\n3. sender_account: Sender’s account ID (string).\n4. receiver_account: Receiver’s account ID (string).\n5. amount: Transaction amount (float).\n6. transaction_type: Type of transaction (e.g., “withdrawal”, “deposit”, “transfer”).\n7. merchant_category: Merchant category (e.g., “utilities”, “online”, “other”).\n8. location: Transaction location (e.g., “Tokyo”, “Toronto”).\n9. device_used: Device used (e.g., “mobile”, “atm”, “pos”).\n10. is_fraud: Fraud label (boolean: False/True).\n11. fraud_type: Type of fraud (string, NaN for non-fraud cases).\n12. time_since_last_transaction: Time since the last transaction (float, likely in seconds or normalized).\n13. spending_deviation_score: Score indicating deviation from typical spending (float).\n14. velocity_score: Transaction velocity score (integer).\n15. geo_anomaly_score: Score for geographic anomalies (float).\n16. payment_channel: Payment method (e.g., “card”, “ACH”, “wire_transfer”).\n17. ip_address: IP address of the transaction (string).\n18. device_hash: Unique device identifier (string).\n\n## Key Observations:\n\n1. Target Variable: is_fraud (boolean) is the label for fraud detection.\n2. Numerical Features: amount, time_since_last_transaction, spending_deviation_score, velocity_score, geo_anomaly_score.\n3. Categorical Features: transaction_type, merchant_category, location, device_used, payment_channel.\n4. Temporal Feature: timestamp (needs conversion to datetime).\n5. Potential Features for EDA: fraud_type (for fraud cases), ip_address, device_hash (for patterns, though likely high cardinality).\n6. Missing Values: fraud_type is NaN for non-fraud cases, which is expected. We’ll check others with data.isnull().sum().","metadata":{}},{"cell_type":"code","source":"# Display basic information\nprint(\"\\nDataset Info:\")\nprint(data.info())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check dataset shape\nprint(\"\\nDataset Shape:\", data.shape)\n\n# Check for missing values\nprint(\"\\nMissing Values:\")\nprint(data.isnull().sum())\n\n# Summary statistics for numerical columns\nprint(\"\\nSummary Statistics:\")\ndata.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Summary Statistics (numerical columns):\n1. amount: Ranges from 0.01 to 3,520.57, mean ~358.93, highly skewed (std ~469.93). Fraud transactions may involve extreme amounts.\n2. time_since_last_transaction: Ranges from -8,777.81 to 8,757.76, mean ~1.53. Negative values are unusual (possibly data errors or reversed time differences). Missing values need handling.\n3. spending_deviation_score: Mean ~0, std ~1, range -5.26 to 5.02. Likely a normalized score, useful for fraud detection.\n4. velocity_score: Integer, range 1 to 20, mean ~10.5. Higher values may indicate rapid transactions, a fraud signal.\n5. geo_anomaly_score: Range 0 to 1, mean ~0.5. Likely a probability-like score, critical for fraud analysis.","metadata":{}},{"cell_type":"code","source":"# Investigate missing time_since_last_transaction\nprint(\"\\nMissing time_since_last_transaction Analysis:\")\nprint(\"Fraud rate in rows with missing time_since_last_transaction:\")\n\nmissing_time_fraud = data[data['time_since_last_transaction'].isnull()]['is_fraud'].mean() * 100\nnon_missing_time_fraud = data[data['time_since_last_transaction'].notnull()]['is_fraud'].mean() * 100","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Missing: {missing_time_fraud:.2f}%\")\nprint(f\"Non-Missing: {non_missing_time_fraud:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert timestamp to datetime\ndata['timestamp'] = pd.to_datetime(data['timestamp'], format='ISO8601')\n\n# Extract hour\ndata['hour'] = data['timestamp'].dt.hour","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"def plot_amount_histogram():\n    plt.figure(figsize=(12, 6))\n    sns.histplot(data=data[data['is_fraud'] == False], x='amount', kde=True, \n                 label='Non-Fraud', stat='density', bins=50, log_scale=True, alpha=0.5)\n    sns.histplot(data=data[data['is_fraud'] == True], x='amount', kde=True, \n                 label='Fraud', stat='density', bins=50, log_scale=True, alpha=0.5)\n    sns.rugplot(data=data[data['is_fraud'] == True], x='amount', color='red', alpha=0.1)\n    \n    plt.title('Distribution of Transaction Amounts (Log Scale)', fontsize=14, pad=10)\n    plt.xlabel('Transaction Amount (USD, Log Scale)', fontsize=12)\n    plt.ylabel('Density', fontsize=12)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_amount_boxplot():\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x='is_fraud', y='amount', data=data, showfliers=False)\n    sns.stripplot(x='is_fraud', y='amount', data=data[data['is_fraud'] == True], \n                  color='red', alpha=0.2, size=3, jitter=True)\n    \n    plt.yscale('log')\n    plt.title('Transaction Amounts by Fraud Status (Log Scale)', fontsize=14, pad=10)\n    plt.xlabel('Fraud Status', fontsize=12)\n    plt.ylabel('Transaction Amount (USD, Log Scale)', fontsize=12)\n    plt.xticks([0, 1], ['Non-Fraud', 'Fraud'])\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_hourly_fraud_rate():\n    hourly_fraud = data.groupby('hour')['is_fraud'].mean() * 100\n    hourly_volume = data.groupby('hour').size()\n    \n    # Calculate confidence intervals (binomial)\n    ci = []\n    for hour in hourly_fraud.index:\n        n = hourly_volume[hour]\n        p = hourly_fraud[hour] / 100\n        se = np.sqrt(p * (1 - p) / n)\n        ci.append(stats.norm.interval(0.95, loc=p * 100, scale=se * 100))\n    ci = np.array(ci)\n    \n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=hourly_fraud.index, y=hourly_fraud.values, \n                             mode='lines+markers', name='Fraud Rate',\n                             line=dict(color='royalblue')))\n    fig.add_trace(go.Scatter(x=hourly_fraud.index, y=ci[:, 0], \n                             mode='lines', name='95% CI Lower', \n                             line=dict(color='lightblue', dash='dash'), showlegend=False))\n    fig.add_trace(go.Scatter(x=hourly_fraud.index, y=ci[:, 1], \n                             mode='lines', name='95% CI Upper', \n                             line=dict(color='lightblue', dash='dash'), fill='tonexty'))\n    \n    # Add annotations for peak and dip\n    fig.add_annotation(x=12, y=3.66, text='Peak (3.66%)', showarrow=True, arrowhead=2)\n    fig.add_annotation(x=20, y=3.51, text='Dip (3.51%)', showarrow=True, arrowhead=2)\n    \n    # Add secondary y-axis for volume\n    fig.add_trace(go.Bar(x=hourly_volume.index, y=hourly_volume.values, \n                        name='Transaction Volume', opacity=0.3, yaxis='y2'))\n    \n    fig.update_layout(\n        title='Hourly Fraud Rate with Transaction Volume',\n        xaxis_title='Hour of Day',\n        yaxis_title='Fraud Rate (%)',\n        yaxis2=dict(title='Transaction Volume', overlaying='y', side='right'),\n        legend=dict(x=0.01, y=0.99),\n        template='plotly_white'\n    )\n    fig.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_categorical_fraud():\n    plt.figure(figsize=(12, 6))\n    fraud_by_type = data.groupby(['transaction_type', 'is_fraud']).size().unstack().fillna(0)\n    fraud_by_type = fraud_by_type.div(fraud_by_type.sum(axis=1), axis=0) * 100\n    \n    fraud_by_type.plot(kind='bar', stacked=True, figsize=(12, 6))\n    plt.title('Fraud Proportion by Transaction Type', fontsize=14, pad=10)\n    plt.xlabel('Transaction Type', fontsize=12)\n    plt.ylabel('Proportion (%)', fontsize=12)\n    plt.legend(['Non-Fraud', 'Fraud'], title='Fraud Status')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_location_device_heatmap():\n    heatmap_data = data.pivot_table(values='is_fraud', index='location', \n                                   columns='device_used', aggfunc='mean') * 100\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='RdBu_r', \n                cbar_kws={'label': 'Fraud Rate (%)'})\n    plt.title('Fraud Rate by Location and Device Used', fontsize=14, pad=10)\n    plt.xlabel('Device Used', fontsize=12)\n    plt.ylabel('Location', fontsize=12)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_correlation_heatmap():\n    numerical_cols = ['amount', 'spending_deviation_score', 'velocity_score', \n                     'geo_anomaly_score', 'time_since_last_transaction']\n    corr_matrix = data[numerical_cols].corr()\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n                vmin=-1, vmax=1, center=0)\n    plt.title('Correlation Matrix of Numerical Features', fontsize=14, pad=10)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_fraud_volume_timeseries():\n    data['date'] = data['timestamp'].dt.date\n    fraud_volume = data[data['is_fraud'] == True].groupby('date').size()\n    fraud_volume = fraud_volume.rolling(window=7, min_periods=1).mean()\n    \n    plt.figure(figsize=(12, 6))\n    fraud_volume.plot()\n    plt.title('7-Day Rolling Average of Fraudulent Transactions', fontsize=14, pad=10)\n    plt.xlabel('Date', fontsize=12)\n    plt.ylabel('Number of Fraudulent Transactions', fontsize=12)\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_amount_histogram()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_amount_boxplot()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_hourly_fraud_rate()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_categorical_fraud()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_location_device_heatmap()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_correlation_heatmap()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_fraud_volume_timeseries()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Modified Feature Engineering and Pipeline\ndef create_features(data):\n    \"\"\"Enhanced feature engineering with categorical handling\"\"\"\n    # Time features\n    data['timestamp'] = pd.to_datetime(data['timestamp'])\n    data['hour'] = data['timestamp'].dt.hour\n    data['day_of_week'] = data['timestamp'].dt.dayofweek\n    data['day_of_month'] = data['timestamp'].dt.day\n    data['month'] = data['timestamp'].dt.month\n    data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)\n    data['is_night'] = ((data['hour'] >= 22) | (data['hour'] <= 6)).astype(int)\n    \n    # Transaction features\n    sender_freq = data['sender_account'].value_counts().to_dict()\n    data['sender_transaction_count'] = data['sender_account'].map(sender_freq)\n    data['amount_log'] = np.log1p(data['amount'])\n    data['amount_to_avg'] = data['amount'] / data.groupby('sender_account')['amount'].transform('mean')\n    \n    # Anomaly scores\n    data['combined_anomaly_score'] = data['spending_deviation_score'] * data['geo_anomaly_score']\n    data['velocity_geo_score'] = data['velocity_score'] * data['geo_anomaly_score']\n    data['time_since_last_transaction'].fillna(-1, inplace=True)\n    data['time_since_last_transaction_log'] = np.log1p(data['time_since_last_transaction'].clip(0))\n    \n    # Identify categorical columns (excluding those we'll drop)\n    cat_cols = ['transaction_type', 'merchant_category', 'location', \n               'device_used', 'payment_channel']\n    \n    # Columns to drop\n    drop_cols = ['transaction_id', 'timestamp', 'sender_account', \n                'receiver_account', 'fraud_type', 'ip_address', \n                'device_hash', 'date']\n    \n    # Separate features before encoding\n    X = data.drop(columns=['is_fraud'] + drop_cols, errors='ignore')\n    y = data['is_fraud']\n    \n    return X, y, cat_cols\n\n# Get features and target\nX, y, categorical_cols = create_features(data)\n\n# Preprocessing Pipeline ===================================================\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Identify numeric columns (exclude categoricals and target)\nnumeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n# Create preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_cols),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n    ])\n\n# 2. Data Resampling ======================================================\ndef balance_data(X, y):\n    \"\"\"Modified resampling that preserves column structure\"\"\"\n    X_fraud = X[y == 1]\n    X_nonfraud = X[y == 0]\n    \n    n_samples = int(len(X_nonfraud) * 0.1)\n    X_fraud_upsampled = resample(X_fraud, n_samples=n_samples, random_state=42)\n    X_nonfraud_downsampled = resample(X_nonfraud, \n                                    n_samples=int(len(X_nonfraud)*0.5), \n                                    random_state=42)\n    \n    X_resampled = pd.concat([X_nonfraud_downsampled, X_fraud_upsampled])\n    y_resampled = pd.Series([0]*len(X_nonfraud_downsampled) + [1]*len(X_fraud_upsampled))\n    \n    idx = np.random.permutation(len(X_resampled))\n    return X_resampled.iloc[idx], y_resampled.iloc[idx]\n\nX_resampled, y_resampled = balance_data(X, y)\n\n# 3. Model Training =======================================================\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n\n# Create full pipeline\nmodel_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier(\n        warm_start=True,\n        random_state=42,\n        class_weight='balanced',\n        n_jobs=1\n    ))\n])\n\n# Hyperparameter tuning\nparam_distributions = {\n    'classifier__n_estimators': [80, 100, 120],\n    'classifier__max_depth': [10, 15, None],\n    'classifier__min_samples_split': [2, 3]\n}\n\nsearch = HalvingRandomSearchCV(\n    model_pipeline,\n    param_distributions,\n    factor=2,\n    cv=3,\n    scoring='average_precision',\n    n_jobs=1,\n    verbose=2,\n    random_state=42,\n    aggressive_elimination=True\n).fit(X_train, y_train)\n\n# 4. Evaluation ==========================================================\nbest_model = search.best_estimator_\n\n# Evaluate on test set\nprint(\"\\n=== Test Set Evaluation ===\")\ny_pred = best_model.predict(X_test)\ny_prob = best_model.predict_proba(X_test)[:, 1]\n\nprint(\"Best Parameters:\", search.best_params_)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\nprint(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\nprint(\"Average Precision:\", average_precision_score(y_test, y_prob))\n\n# Feature Importance (requires getting feature names from preprocessor)\nonehot_columns = best_model.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_cols)\nall_features = numeric_cols + list(onehot_columns)\n\nimportances = best_model.named_steps['classifier'].feature_importances_\nfeat_imp = pd.DataFrame({'Feature': all_features, 'Importance': importances}).sort_values('Importance', ascending=False)\n\nplt.figure(figsize=(12, 8))\nsns.barplot(x='Importance', y='Feature', data=feat_imp.head(20))\nplt.title(\"Top 20 Feature Importances\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}